from pyspark.sql import SparkSession
from pyspark.sql.functions import broadcast, col
import os

# --- Configuration ---

# --- S3 Paths ---
# Path to the large dataset generated by Athena UNLOAD/CTAS
MAIN_DATA_S3_PATH = "s3a://your-output-data-bucket/unloaded_data/" # Or the CTAS path
# Path to the smaller, static dataset file/directory
STATIC_DATA_S3_PATH = "s3a://your-static-data-bucket/static-reference-data/"
# Path for the final merged output
FINAL_OUTPUT_S3_PATH = "s3a://your-final-output-bucket/merged-analysis-results/"

# --- Join Configuration ---
# Column(s) to join the two datasets on
JOIN_KEY_COLUMN = "common_id_column" # Replace with your actual key column name
# Or for multiple columns: JOIN_KEY_COLUMNS = ["key_col1", "key_col2"]
JOIN_TYPE = "left" # e.g., "inner", "left", "right", "full_outer"

# --- Optional Output Configuration ---
# Partition the final output by these columns for faster downstream reads?
# Set to None or [] to disable output partitioning.
OUTPUT_PARTITION_COLUMNS = ["year", "month"] # Example: if you have year/month columns
# Number of output partitions (files). Adjust based on data size and downstream needs.
# Use None to let Spark decide (based on spark.sql.shuffle.partitions)
# Use 1 if you need a single file (only for relatively small final output)
NUM_OUTPUT_PARTITIONS = None # Example: 200

# --- Spark Session Setup ---
# Note: In production environments (EMR, Databricks), SparkSession is often pre-configured.
# These configurations are examples; tune them based on your cluster/environment.
spark = (
    SparkSession.builder
    .appName("LargeDataMerge")
    # --- Important Configurations for S3 & Performance ---
    # 1. S3A Connector (Ensure JARs are on classpath)
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    # Use IAM roles in production (preferred):
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.InstanceProfileCredentialsProvider")
    # Or uncomment and set keys for local testing (less secure):
    # .config("spark.hadoop.fs.s3a.access.key", "YOUR_ACCESS_KEY")
    # .config("spark.hadoop.fs.s3a.secret.key", "YOUR_SECRET_KEY")
    # 2. Memory Settings (Adjust based on cluster nodes)
    .config("spark.executor.memory", "8g")
    .config("spark.driver.memory", "4g")
    # 3. Shuffle Partitions (Crucial for join performance if broadcast fails or isn't used)
    .config("spark.sql.shuffle.partitions", "200") # Adjust based on data size/cores
    # 4. Parquet Optimizations (Usually enabled by default)
    .config("spark.sql.parquet.filterPushdown", "true")
    .config("spark.sql.parquet.enableVectorizedReader", "true")
    # --- Add other necessary configurations ---
    # .master("local[*]") # For local testing
    .getOrCreate()
)

# Set log level (optional)
spark.sparkContext.setLogLevel("INFO")
log = spark.sparkContext._jvm.org.apache.log4j
logger = log.LogManager.getLogger("LargeDataMergeApp")
logger.info("Spark Session Created. Starting data processing...")

try:
    # --- 1. Read the Main Large Dataset ---
    logger.info(f"Reading main dataset from: {MAIN_DATA_S3_PATH}")
    main_df = spark.read.parquet(MAIN_DATA_S3_PATH)
    logger.info(f"Main dataset schema:")
    main_df.printSchema()
    logger.info(f"Main dataset count (triggering action): {main_df.count()}") # Optional: for verification

    # --- 2. Read the Smaller Static Dataset ---
    logger.info(f"Reading static dataset from: {STATIC_DATA_S3_PATH}")
    static_df = spark.read.parquet(STATIC_DATA_S3_PATH)
    logger.info(f"Static dataset schema:")
    static_df.printSchema()
    # Optional: Cache if reused multiple times, but broadcast usually handles memory well
    # static_df.cache()
    static_count = static_df.count() # Action to estimate size / verify read
    logger.info(f"Static dataset count: {static_count}")

    # --- 3. Perform the Merge (Join) ---
    # **Optimization:** Use broadcast hint on the smaller DataFrame (static_df)
    # This avoids shuffling the large main_df. Spark might do this automatically
    # based on statistics, but explicitly hinting is safer.
    logger.info(f"Performing '{JOIN_TYPE}' join on column(s): {JOIN_KEY_COLUMN}")

    # Handle potential column name collisions (if static_df has columns also in main_df, besides the key)
    # Rename static columns before join, except the join key
    static_cols_renamed = [
        col(c).alias(f"static_{c}") if c != JOIN_KEY_COLUMN else col(c)
        for c in static_df.columns
    ]
    static_df_renamed = static_df.select(static_cols_renamed)
    logger.info("Renamed static columns (prefix 'static_') to avoid collision.")
    static_df_renamed.printSchema()


    # Perform the join using the broadcast hint
    merged_df = main_df.join(
        broadcast(static_df_renamed), # Apply broadcast hint here
        on=JOIN_KEY_COLUMN, # Specify the join key
        how=JOIN_TYPE       # Specify the join type
    )

    logger.info("Join completed. Schema of merged data:")
    merged_df.printSchema()

    # Optional: Select only necessary columns for the final output
    # merged_df = merged_df.select("col1", "col2", "static_col_a", ...)

    # --- 4. Write the Final Merged Dataset ---
    logger.info(f"Writing final merged dataset to: {FINAL_OUTPUT_S3_PATH}")

    writer = merged_df.write.format("parquet").mode("overwrite") # Use "append" if needed

    # **Optimization:** Partition output data if beneficial for downstream queries
    if OUTPUT_PARTITION_COLUMNS:
        logger.info(f"Partitioning output by: {OUTPUT_PARTITION_COLUMNS}")
        writer = writer.partitionBy(*OUTPUT_PARTITION_COLUMNS)

    # **Optimization:** Control number of output files (partitions)
    # Repartitioning involves a shuffle, coalesce avoids full shuffle but can be skewed.
    # Only use if you have specific needs for file count/size.
    final_df_to_write = merged_df
    if NUM_OUTPUT_PARTITIONS and NUM_OUTPUT_PARTITIONS > 0:
         logger.info(f"Repartitioning output to {NUM_OUTPUT_PARTITIONS} partitions.")
         # Use repartition for more even distribution (involves shuffle)
         final_df_to_write = merged_df.repartition(NUM_OUTPUT_PARTITIONS)
         # Or use coalesce to reduce partitions with less shuffling (can lead to skew)
         # final_df_to_write = merged_df.coalesce(NUM_OUTPUT_PARTITIONS)


    # Execute the write operation
    writer.save(FINAL_OUTPUT_S3_PATH)

    logger.info("Successfully wrote final dataset.")

except Exception as e:
    logger.error(f"An error occurred during processing: {e}", exc_info=True)
    raise # Re-raise the exception after logging

finally:
    # --- Stop the Spark Session ---
    logger.info("Stopping Spark Session.")
    spark.stop()
