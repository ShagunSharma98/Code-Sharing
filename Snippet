from pyspark.sql import SparkSession
from pyspark.sql.functions import broadcast

# Initialize Spark session
spark = SparkSession.builder \
    .appName("S3 Parquet Merge Job") \
    .config("spark.sql.shuffle.partitions", "200") \  # adjust based on your cluster
    .config("spark.executor.memory", "8g") \           # adjust based on your infra
    .config("spark.executor.cores", "4") \             # adjust for optimization
    .getOrCreate()

# S3 Paths
main_dataset_path = "s3://your-bucket-name/path/to/main_dataset/"
static_dataset_path = "s3://your-bucket-name/path/to/static_dataset/"
final_output_path = "s3://your-bucket-name/path/to/final_output/"

# Read the main dataset (parquet)
main_df = spark.read.parquet(main_dataset_path)

# Read the static dataset (assuming CSV for example; adjust if Parquet/other)
static_df = spark.read.option("header", "true").csv(static_dataset_path)

# Optimize: Broadcast the smaller dataset if it is small enough
# If static_df is small (<500MB), broadcasting makes joins much faster
merged_df = main_df.join(broadcast(static_df), on="common_key_column", how="inner")

# Optional: Repartition before saving to control file sizes
final_df = merged_df.repartition(200)  # adjust based on desired output file size

# Save the final DataFrame back to S3 as Parquet
final_df.write.mode("overwrite").parquet(final_output_path)

# Stop Spark session
spark.stop()
